{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install albumentations --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.4\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    " \n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "# %pip install --force-reinstall pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliente\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
    "# !pip install segmentation-models-pytorch\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dat_image_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_0.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_0_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_100.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_100_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_101.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_101_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_102.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_102_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_103.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_103_m.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                        dat_image_path  \\\n",
       "0         0    data/patches\\M-33-20-D-c-4-2_0.jpg   \n",
       "1         1  data/patches\\M-33-20-D-c-4-2_100.jpg   \n",
       "2         2  data/patches\\M-33-20-D-c-4-2_101.jpg   \n",
       "3         3  data/patches\\M-33-20-D-c-4-2_102.jpg   \n",
       "4         4  data/patches\\M-33-20-D-c-4-2_103.jpg   \n",
       "\n",
       "                                mask_path  \n",
       "0    data/patches\\M-33-20-D-c-4-2_0_m.png  \n",
       "1  data/patches\\M-33-20-D-c-4-2_100_m.png  \n",
       "2  data/patches\\M-33-20-D-c-4-2_101_m.png  \n",
       "3  data/patches\\M-33-20-D-c-4-2_102_m.png  \n",
       "4  data/patches\\M-33-20-D-c-4-2_103_m.png  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'data/patches'\n",
    "\n",
    "# ler as pastas masks e images e criar um csv com os caminhos\n",
    "def create_df(data_path):\n",
    "    data = []\n",
    "    index = 0\n",
    "    for image in os.listdir(data_path):\n",
    "      # se a imagem tiver _m antes do .png, é uma máscara\n",
    "      if '_m' in image:\n",
    "        mask_path = os.path.join(data_path, image)\n",
    "        sat_image = image.replace('_m', '')\n",
    "        sat_image = sat_image.replace('.png', '.jpg')\n",
    "        sat_image_path = os.path.join(data_path, sat_image)\n",
    "        data.append([index, sat_image_path, mask_path])\n",
    "        index += 1\n",
    "    return pd.DataFrame(data, columns=['image_id', 'dat_image_path', 'mask_path'])\n",
    "  \n",
    "df = create_df(DATA_DIR)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvar o csv\n",
    "df.to_csv('metadata_patches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = cv2.imread(df['mask_path'][110], cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels:\n",
    "# 0: Unlabeled background ​\n",
    "# 1: Buildings​\n",
    "# 2: Woodlands​\n",
    "# 3: Water​\n",
    "\n",
    "# create csv with the labels\n",
    "def create_label_csv():\n",
    "    data = [[0, 'Unlabeled background'], [1, 'Buildings'], [2, 'Woodlands'], [3, 'Water']]\n",
    "    return pd.DataFrame(data, columns=['label', 'name'])\n",
    "  \n",
    "label_df = create_label_csv()\n",
    "label_df.to_csv('class_dict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_0.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_0_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_1.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_1_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_10.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_10_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_100.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_100_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_102.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_102_m.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image  \\\n",
       "0    data/patches\\M-33-20-D-c-4-2_0.jpg   \n",
       "1    data/patches\\M-33-20-D-c-4-2_1.jpg   \n",
       "2   data/patches\\M-33-20-D-c-4-2_10.jpg   \n",
       "3  data/patches\\M-33-20-D-c-4-2_100.jpg   \n",
       "4  data/patches\\M-33-20-D-c-4-2_102.jpg   \n",
       "\n",
       "                                mask_path  \n",
       "0    data/patches\\M-33-20-D-c-4-2_0_m.png  \n",
       "1    data/patches\\M-33-20-D-c-4-2_1_m.png  \n",
       "2   data/patches\\M-33-20-D-c-4-2_10_m.png  \n",
       "3  data/patches\\M-33-20-D-c-4-2_100_m.png  \n",
       "4  data/patches\\M-33-20-D-c-4-2_102_m.png  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eu tenho um txt com todas as imagens que são para treino, crie um dataframe com essas imagens\n",
    "def create_train_df(data_path, txt_path):\n",
    "    train_images = []\n",
    "    with open(txt_path, 'r') as file:\n",
    "        for line in file:\n",
    "            image = line.strip()\n",
    "            mask_path = os.path.join(data_path, f'{image}_m.png')\n",
    "            image_path = os.path.join(data_path, f\"{image}.jpg\")\n",
    "            train_images.append([image_path, mask_path])\n",
    "    return pd.DataFrame(train_images, columns=['image', 'mask_path'])\n",
    "  \n",
    "train_df = create_train_df(DATA_DIR, 'data/train.txt')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_105.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_105_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_110.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_110_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_117.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_117_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_120.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_120_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_124.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_124_m.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image  \\\n",
       "0  data/patches\\M-33-20-D-c-4-2_105.jpg   \n",
       "1  data/patches\\M-33-20-D-c-4-2_110.jpg   \n",
       "2  data/patches\\M-33-20-D-c-4-2_117.jpg   \n",
       "3  data/patches\\M-33-20-D-c-4-2_120.jpg   \n",
       "4  data/patches\\M-33-20-D-c-4-2_124.jpg   \n",
       "\n",
       "                                mask_path  \n",
       "0  data/patches\\M-33-20-D-c-4-2_105_m.png  \n",
       "1  data/patches\\M-33-20-D-c-4-2_110_m.png  \n",
       "2  data/patches\\M-33-20-D-c-4-2_117_m.png  \n",
       "3  data/patches\\M-33-20-D-c-4-2_120_m.png  \n",
       "4  data/patches\\M-33-20-D-c-4-2_124_m.png  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_test_df(data_path, txt_path):\n",
    "    test_images = []\n",
    "    with open(txt_path, 'r') as file:\n",
    "        for line in file:\n",
    "            image = line.strip()\n",
    "            mask_path = os.path.join(data_path, f'{image}_m.png')\n",
    "            image_path = os.path.join(data_path, f\"{image}.jpg\")\n",
    "            test_images.append([image_path, mask_path])\n",
    "    return pd.DataFrame(test_images, columns=['image', 'mask_path'])\n",
    "  \n",
    "test_df = create_test_df(DATA_DIR, 'data/test.txt')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_101.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_101_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_103.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_103_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_113.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_113_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_116.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_116_m.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_126.jpg</td>\n",
       "      <td>data/patches\\M-33-20-D-c-4-2_126_m.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image  \\\n",
       "0  data/patches\\M-33-20-D-c-4-2_101.jpg   \n",
       "1  data/patches\\M-33-20-D-c-4-2_103.jpg   \n",
       "2  data/patches\\M-33-20-D-c-4-2_113.jpg   \n",
       "3  data/patches\\M-33-20-D-c-4-2_116.jpg   \n",
       "4  data/patches\\M-33-20-D-c-4-2_126.jpg   \n",
       "\n",
       "                                mask_path  \n",
       "0  data/patches\\M-33-20-D-c-4-2_101_m.png  \n",
       "1  data/patches\\M-33-20-D-c-4-2_103_m.png  \n",
       "2  data/patches\\M-33-20-D-c-4-2_113_m.png  \n",
       "3  data/patches\\M-33-20-D-c-4-2_116_m.png  \n",
       "4  data/patches\\M-33-20-D-c-4-2_126_m.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_valid_df(data_path, txt_path):\n",
    "    valid_images = []\n",
    "    with open(txt_path, 'r') as file:\n",
    "        for line in file:\n",
    "            image = line.strip()\n",
    "            mask_path = os.path.join(data_path, f'{image}_m.png')\n",
    "            image_path = os.path.join(data_path, f\"{image}.jpg\")\n",
    "            valid_images.append([image_path, mask_path])\n",
    "    return pd.DataFrame(valid_images, columns=['image', 'mask_path'])\n",
    "  \n",
    "valid_df = create_valid_df(DATA_DIR, 'data/val.txt')\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7470\n",
      "Test size: 1602\n",
      "Validation size: 1602\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_df.shape[0]}\")\n",
    "print(f\"Test size: {test_df.shape[0]}\")\n",
    "print(f\"Validation size: {valid_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class LandcoverDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      image_path = self.df['image'].iloc[idx]\n",
    "      mask_path = self.df['mask_path'].iloc[idx]\n",
    "      \n",
    "      image = cv2.imread(image_path)\n",
    "      mask = cv2.imread(mask_path)\n",
    "\n",
    "      if self.transform:\n",
    "          label = mask[:, :, 1]\n",
    "          transformed = self.transform(image=image, mask=label)\n",
    "          img = np.transpose(transformed[\"image\"], (2, 0, 1))\n",
    "          label = transformed[\"mask\"]\n",
    "      else:\n",
    "          img = np.transpose(image, (2, 0, 1))\n",
    "          label = mask[:, :, 1]\n",
    "\n",
    "      # Adicionar dimensão extra para a máscara\n",
    "      label = np.expand_dims(label, axis=0)\n",
    "      \n",
    "      del mask\n",
    "      return torch.tensor(img, dtype=torch.float32) / 255, torch.tensor(label, dtype=torch.int64)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation ():\n",
    "    train_transform = [\n",
    "        album.OneOf([\n",
    "            album.HueSaturationValue(40,40,30,p=1),\n",
    "            album.RandomBrightnessContrast(p=1,brightness_limit = 0.2,\n",
    "                                        contrast_limit = 0.5)], p = 0.5),\n",
    "        album.OneOf([\n",
    "            album.RandomRotate90(p=1),\n",
    "            album.HorizontalFlip(p=1),\n",
    "            album.RandomSizedCrop(min_max_height=(248,512),height=512,width=512, p =1)\n",
    "            ], p = 0.5)\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "  \n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['buildings', 'woodlands', 'water', 'background', 'road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LandcoverDataset(train_df, transform = get_augmentation())\n",
    "test_set = LandcoverDataset(test_df)\n",
    "valid_set = LandcoverDataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dloader = DataLoader(train_set, batch_size = batch_size,\n",
    "                           shuffle = True, num_workers = 0)\n",
    "test_dloader = DataLoader(test_set, batch_size = batch_size, num_workers = 0)\n",
    "val_dloader = DataLoader(valid_set, batch_size=batch_size, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=5e-5),\n",
    "])\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(),\n",
    "    smp.utils.metrics.Fscore(),\n",
    "    smp.utils.metrics.Accuracy()\n",
    "]\n",
    "\n",
    "# create epoch runners\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "def training_loop(model, train_loader, val_loader, epochs,\n",
    "                  lr, loss_fn, regularization=None,\n",
    "                  reg_lambda=None, mod_epochs=5, early_stopping = False,\n",
    "                  patience = None, verbose = None, model_title = None, save = None,\n",
    "                 stopping_criterion = \"loss\"):\n",
    "    if stopping_criterion not in [\"loss\",\"IoU\"]:\n",
    "        raise ValueError(f\"Critério de parada deve ser 'loss' ou 'IoU', nao {stopping_criterion}.\")\n",
    "    print(\"Treinamento de \" + model_title + \" iniciou!\")\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    if stopping_criterion == \"IoU\":\n",
    "        jaccard = torchmetrics.JaccardIndex(num_classes = 5).to(device)\n",
    "\n",
    "        iou_loss_list = []\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    num_train_batches = len(train_loader)\n",
    "    num_val_batches = len(val_loader)\n",
    "    counter_epochs = 0\n",
    "\n",
    "    if early_stopping:\n",
    "        ear_stopping = EarlyStopping(patience= patience, verbose=verbose)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        counter_epochs+=1\n",
    "        model.train()\n",
    "        train_loss, val_loss = 0.0, 0.0\n",
    "        \n",
    "        if stopping_criterion == \"IoU\": iou_loss = 0.0 \n",
    "        for train_batch in train_loader:\n",
    "            X, y = train_batch[0].to(device), train_batch[1].to(device)\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "            train_loss += loss.item()\n",
    "            if stopping_criterion == \"IoU\":\n",
    "                iou_loss += 1- float(jaccard(preds, y).cpu())\n",
    "\n",
    "            # Regulirization\n",
    "            if regularization == 'L2':\n",
    "                l_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                loss = loss + reg_lambda * l_norm\n",
    "            elif regularization == 'L1':\n",
    "                l_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + reg_lambda * l_norm\n",
    "\n",
    "            # Backpropagation\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                X, y = val_batch[0].to(device), val_batch[1].to(device)\n",
    "                preds = model(X)\n",
    "                if stopping_criterion == \"loss\":\n",
    "                    val_loss += loss_fn(preds, y).item()\n",
    "                else:\n",
    "                    # Calculate the 1-IoU as validation loss\n",
    "                    val_loss += 1-float(jaccard(preds,y).cpu())\n",
    "        train_loss /= num_train_batches\n",
    "        val_loss /= num_val_batches\n",
    "        if stopping_criterion == \"IoU\":\n",
    "            iou_loss /= num_train_batches\n",
    "            iou_loss_list.append(iou_loss)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        if (epoch + 1) % mod_epochs == 0:\n",
    "            if stopping_criterion == \"loss\":\n",
    "                print(\n",
    "                    f\"ÉPOCA: {epoch + 1}/{epochs}{5 * ' '} Loss de treino: {train_loss:.4f}{5 * ' '} Loss de validacao: {val_loss:.4f}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"ÉPOCA: {epoch + 1}/{epochs}{5 * ' '} IoU Loss de treino: {iou_loss:.4f}{5 * ' '} IoU Loss de validacao: {val_loss:.4f}{5*' '} Loss de treino: {train_loss:.4f}\")\n",
    "                \n",
    "                \n",
    "\n",
    "        if early_stopping:\n",
    "            ear_stopping(val_loss, model)\n",
    "            if ear_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    sns.set_style(\"dark\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    if stopping_criterion == \"loss\":\n",
    "        ax.plot(range(1, counter_epochs + 1), train_loss_list, label='Train Loss',\n",
    "               color = \"#808080\", linewidth = 2.5)\n",
    "    else:\n",
    "        ax.plot(range(1, counter_epochs + 1), iou_loss_list, label='Train IoU Loss',\n",
    "               color = \"#808080\", linewidth = 2.5)\n",
    "    ax.plot(range(1, counter_epochs + 1), val_loss_list, label='Val Loss',\n",
    "            color = \"#36454F\", linewidth = 2.5)\n",
    "    ax.set_title(model_title, fontsize = 15)\n",
    "    ax.set_ylabel(\"Loss\", fontsize = 13)\n",
    "    ax.set_xlabel(\"Epochs\", fontsize = 13)\n",
    "    plt.legend()\n",
    "    if save is not None:\n",
    "        plt.savefig(model_title + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "    if early_stopping:\n",
    "        model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    total_time = time.time() - tic\n",
    "    mins, secs = divmod(total_time, 60)\n",
    "    if mins < 60:\n",
    "        print(f\"\\n Training completed in {mins} m {secs:.2f} s.\")\n",
    "    else:\n",
    "        hours, mins = divmod(mins, 60)\n",
    "        print(f\"\\n Training completed in {hours} h {mins} m {secs:.2f} s.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of DeepLabV3+ with Resnet50 encoder starts!\n",
      "Using loss as stopping criterion.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliente\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50     Training Loss: 0.0484     Validation Loss: 0.1466\n",
      "Validation loss decreased (inf --> 0.146648).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# train model for 50 epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "lr = 5e-5\n",
    "\n",
    "training_loop(model, train_dloader, val_dloader, EPOCHS, lr, loss, mod_epochs =1,\n",
    "             regularization = \"L2\", reg_lambda = 1e-6, early_stopping = True,\n",
    "             patience = 5, verbose = True, model_title = \"DeepLabV3+ with Resnet50 encoder\", save = True,\n",
    "             stopping_criterion = \"loss\")\n",
    "# training_loop(model, train_dloader, val_dloader, EPOCHS, loss, model_title = \"DeepLabV3Plus\", model_path = \"DeepLabV3Plus.pth\", stopping_criterion = \"loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
